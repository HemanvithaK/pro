{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "39kvJoUx7tpt"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Columns loaded: ['Unnamed: 0', 'key', 'fare_amount', 'pickup_datetime', 'pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude', 'passenger_count']\n",
            "\n",
            "First 5 rows of the DataFrame:\n",
            "   Unnamed: 0                            key  fare_amount  \\\n",
            "0    24238194    2015-05-07 19:52:06.0000003          7.5   \n",
            "1    27835199    2009-07-17 20:04:56.0000002          7.7   \n",
            "2    44984355   2009-08-24 21:45:00.00000061         12.9   \n",
            "3    25894730    2009-06-26 08:22:21.0000001          5.3   \n",
            "4    17610152  2014-08-28 17:47:00.000000188         16.0   \n",
            "\n",
            "           pickup_datetime  pickup_longitude  pickup_latitude  \\\n",
            "0  2015-05-07 19:52:06 UTC        -73.999817        40.738354   \n",
            "1  2009-07-17 20:04:56 UTC        -73.994355        40.728225   \n",
            "2  2009-08-24 21:45:00 UTC        -74.005043        40.740770   \n",
            "3  2009-06-26 08:22:21 UTC        -73.976124        40.790844   \n",
            "4  2014-08-28 17:47:00 UTC        -73.925023        40.744085   \n",
            "\n",
            "   dropoff_longitude  dropoff_latitude  passenger_count  \n",
            "0         -73.999512         40.723217                1  \n",
            "1         -73.994710         40.750325                1  \n",
            "2         -73.962565         40.772647                1  \n",
            "3         -73.965316         40.803349                3  \n",
            "4         -73.973082         40.761247                5  \n",
            "------------------------------\n",
            "Training Linear Regression...\n",
            "Training Decision Tree...\n",
            "Training Random Forest...\n",
            "Training Gradient Boosting...\n",
            "\n",
            "========================================\n",
            "Model Performance Comparison:\n",
            "========================================\n",
            "\n",
            "--- Linear Regression ---\n",
            "  Mean Absolute Error (MAE): 5.99\n",
            "  Mean Squared Error (MSE): 102.29\n",
            "  Root Mean Squared Error (RMSE): 10.11\n",
            "  Median Absolute Error (MedAE): 4.42\n",
            "  R-squared: 0.02\n",
            "  Mean Absolute Percentage Error (MAPE): 64.30%\n",
            "\n",
            "--- Decision Tree ---\n",
            "  Mean Absolute Error (MAE): 3.26\n",
            "  Mean Squared Error (MSE): 55.90\n",
            "  Root Mean Squared Error (RMSE): 7.48\n",
            "  Median Absolute Error (MedAE): 1.60\n",
            "  R-squared: 0.46\n",
            "  Mean Absolute Percentage Error (MAPE): 32.62%\n",
            "\n",
            "--- Random Forest ---\n",
            "  Mean Absolute Error (MAE): 2.41\n",
            "  Mean Squared Error (MSE): 32.84\n",
            "  Root Mean Squared Error (RMSE): 5.73\n",
            "  Median Absolute Error (MedAE): 1.33\n",
            "  R-squared: 0.68\n",
            "  Mean Absolute Percentage Error (MAPE): 24.74%\n",
            "\n",
            "--- Gradient Boosting ---\n",
            "  Mean Absolute Error (MAE): 2.26\n",
            "  Mean Squared Error (MSE): 31.57\n",
            "  Root Mean Squared Error (RMSE): 5.62\n",
            "  Median Absolute Error (MedAE): 1.26\n",
            "  R-squared: 0.70\n",
            "  Mean Absolute Percentage Error (MAPE): 22.51%\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, median_absolute_error\n",
        "import numpy as np\n",
        "\n",
        "# --- Modify this line to load your CSV file ---\n",
        "# The filename is 'uber.csv'\n",
        "# Changed sep='\\t' to sep=',' because the output showed comma separation.\n",
        "try:\n",
        "    df = pd.read_csv('uber.csv', sep=',') # Changed separator here!\n",
        "\n",
        "    # --- Added lines to check the loaded data (keep these for verification) ---\n",
        "    print(\"Columns loaded:\", df.columns.tolist())\n",
        "    print(\"\\nFirst 5 rows of the DataFrame:\")\n",
        "    print(df.head())\n",
        "    print(\"-\" * 30) # Separator for clarity\n",
        "    # --- End of added lines ---\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'uber.csv' not found. Please make sure the CSV file is in the correct directory.\")\n",
        "    exit() # Exit if the file is not found\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while reading the CSV file: {e}\")\n",
        "    exit()\n",
        "\n",
        "# --- Preprocessing ---\n",
        "# Convert pickup_datetime to datetime objects\n",
        "# We use errors='coerce' to turn any parsing errors into NaT (Not a Time)\n",
        "# Ensure the column exists before attempting to process it\n",
        "if 'pickup_datetime' not in df.columns:\n",
        "    print(\"Error: 'pickup_datetime' column not found in the CSV file after loading.\")\n",
        "    print(\"Please check the column names printed above and the separator used.\")\n",
        "    exit()\n",
        "\n",
        "# Now this line should work as 'pickup_datetime' will be a recognized column\n",
        "df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'].astype(str).str.replace(' UTC', '', regex=False), errors='coerce')\n",
        "\n",
        "# Drop rows where pickup_datetime is NaT after coercion\n",
        "df.dropna(subset=['pickup_datetime'], inplace=True)\n",
        "\n",
        "# Extract features from pickup_datetime\n",
        "df['year'] = df['pickup_datetime'].dt.year\n",
        "df['month'] = df['pickup_datetime'].dt.month\n",
        "df['day'] = df['pickup_datetime'].dt.day\n",
        "df['hour'] = df['pickup_datetime'].dt.hour\n",
        "df['day_of_week'] = df['pickup_datetime'].dt.dayofweek # Monday=0, Sunday=6\n",
        "\n",
        "# Calculate distance between pickup and dropoff coordinates\n",
        "def haversine_distance(lat1, lon1, lat2, lon2):\n",
        "    R = 6371  # Earth radius in kilometers\n",
        "    # Check for NaN before converting to radians\n",
        "    if pd.isna([lat1, lon1, lat2, lon2]).any():\n",
        "        return np.nan\n",
        "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
        "    dlon = lon2 - lon1\n",
        "    dlat = lat2 - lat1\n",
        "    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
        "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))\n",
        "    return R * c\n",
        "\n",
        "# Apply distance calculation\n",
        "# Ensure coordinate columns exist before applying\n",
        "coordinate_cols = ['pickup_latitude', 'pickup_longitude', 'dropoff_latitude', 'dropoff_longitude']\n",
        "# Check column existence after changing separator\n",
        "if not all(col in df.columns for col in coordinate_cols):\n",
        "     print(f\"Error: Missing one or more coordinate columns: {coordinate_cols} after loading with comma separator.\")\n",
        "     print(\"Please check the actual column names and separator in your CSV file.\")\n",
        "     exit()\n",
        "\n",
        "\n",
        "df['distance'] = df.apply(lambda row: haversine_distance(row['pickup_latitude'], row['pickup_longitude'], row['dropoff_latitude'], row['dropoff_longitude']), axis=1)\n",
        "\n",
        "# Drop rows with NaN values created during preprocessing\n",
        "# This includes NaNs from datetime conversion, distance calculation, etc.\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# Ensure the DataFrame is not empty after dropping NaNs\n",
        "if df.empty:\n",
        "    print(\"Error: DataFrame is empty after dropping rows with missing or invalid data.\")\n",
        "    print(\"Please check your data file for completeness and correct formatting.\")\n",
        "    exit()\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "features = ['year', 'month', 'day', 'hour', 'day_of_week', 'passenger_count', 'distance']\n",
        "target = 'fare_amount'\n",
        "\n",
        "# Ensure that the selected features and target columns exist in the DataFrame after preprocessing\n",
        "# This check is redundant if dropna is done, but good practice\n",
        "missing_features = [f for f in features if f not in df.columns]\n",
        "if missing_features:\n",
        "     print(f\"Error: Missing feature column(s) in DataFrame after preprocessing: {missing_features}\")\n",
        "     print(\"Please check your data and preprocessing steps.\")\n",
        "     exit()\n",
        "\n",
        "if target not in df.columns:\n",
        "    print(f\"Error: Target column '{target}' not found in the DataFrame after preprocessing.\")\n",
        "    print(\"Please check your data and preprocessing steps.\")\n",
        "    exit()\n",
        "\n",
        "\n",
        "X = df[features]\n",
        "y = df[target]\n",
        "\n",
        "# Split data into training and testing sets\n",
        "# We check if there's enough data to split\n",
        "if len(df) < 2:\n",
        "    print(\"Error: Not enough data to perform a train-test split. Need at least 2 data points.\")\n",
        "    exit()\n",
        "\n",
        "# Adjust test_size if the dataset is very small, ensuring at least one sample in each set\n",
        "test_size = 0.2\n",
        "if len(df) * test_size < 1 and len(df) > 1:\n",
        "    test_size = 1 / len(df) # Ensure at least one sample in test set if possible\n",
        "if len(df) * (1 - test_size) < 1 and len(df) > 1:\n",
        "     test_size = (len(df) - 1) / len(df) # Ensure at least one sample in training set if possible\n",
        "if test_size >= 1 or test_size <= 0: # Fallback for extremely small datasets (<=1)\n",
        "     if len(df) == 1:\n",
        "          print(\"Warning: Only one data point available. Cannot perform train-test split.\")\n",
        "          exit()\n",
        "     test_size = 0.5 # Default to 50/50 split for small >1 datasets\n",
        "\n",
        "\n",
        "try:\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
        "except ValueError as e:\n",
        "    print(f\"Error during train-test split: {e}\")\n",
        "    print(\"This might happen if the dataset is too small or test_size calculation resulted in an invalid split.\")\n",
        "    exit()\n",
        "\n",
        "\n",
        "# Initialize and train different models\n",
        "models = {\n",
        "    \"Linear Regression\": LinearRegression(),\n",
        "    \"Decision Tree\": DecisionTreeRegressor(random_state=42),\n",
        "    \"Random Forest\": RandomForestRegressor(random_state=42),\n",
        "    \"Gradient Boosting\": GradientBoostingRegressor(random_state=42)\n",
        "}\n",
        "\n",
        "results = {}\n",
        "\n",
        "# Check if test set is empty before making predictions\n",
        "if X_test.empty:\n",
        "    print(\"Error: Test set is empty after splitting. Cannot evaluate models.\")\n",
        "else:\n",
        "    for name, model in models.items():\n",
        "        print(f\"Training {name}...\")\n",
        "        # Ensure there's enough data in training set\n",
        "        if X_train.empty or y_train.empty:\n",
        "             print(f\"Error: Training set is empty for {name}. Cannot train model.\")\n",
        "             continue # Skip to the next model\n",
        "\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_test)\n",
        "\n",
        "        # Calculate metrics\n",
        "        mae = mean_absolute_error(y_test, y_pred)\n",
        "        mse = mean_squared_error(y_test, y_pred) # Added MSE\n",
        "        rmse = np.sqrt(mse) # Calculate RMSE from MSE\n",
        "        medae = median_absolute_error(y_test, y_pred) # Added Median Absolute Error\n",
        "\n",
        "        # Calculate R-squared\n",
        "        if np.std(y_test) == 0:\n",
        "            r2 = np.nan # R-squared is undefined if the target variable has no variance\n",
        "        else:\n",
        "            r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "        # Calculate Mean Absolute Percentage Error (MAPE)\n",
        "        # Avoid division by zero if actual fare is 0\n",
        "        y_test_no_zero = y_test[y_test != 0]\n",
        "        y_pred_no_zero = y_pred[y_test != 0]\n",
        "        if len(y_test_no_zero) > 0:\n",
        "             mape = np.mean(np.abs((y_test_no_zero - y_pred_no_zero) / y_test_no_zero)) * 100\n",
        "        else:\n",
        "             mape = np.nan # Cannot calculate MAPE if all actual values are zero\n",
        "\n",
        "\n",
        "        results[name] = {\n",
        "            \"MAE\": mae,\n",
        "            \"MSE\": mse, # Added MSE\n",
        "            \"RMSE\": rmse,\n",
        "            \"MedAE\": medae, # Added Median Absolute Error\n",
        "            \"R-squared\": r2,\n",
        "            \"MAPE\": mape # Added MAPE\n",
        "            }\n",
        "\n",
        "    # Print comparison of results\n",
        "    print(\"\\n\" + \"=\"*40) # Wider Separator\n",
        "    print(\"Model Performance Comparison:\")\n",
        "    print(\"=\"*40) # Wider Separator\n",
        "    if results: # Only print results if models were evaluated\n",
        "        for name, metrics in results.items():\n",
        "            print(f\"\\n--- {name} ---\")\n",
        "            print(f\"  Mean Absolute Error (MAE): {metrics['MAE']:.2f}\")\n",
        "            print(f\"  Mean Squared Error (MSE): {metrics['MSE']:.2f}\") # Print MSE\n",
        "            print(f\"  Root Mean Squared Error (RMSE): {metrics['RMSE']:.2f}\")\n",
        "            print(f\"  Median Absolute Error (MedAE): {metrics['MedAE']:.2f}\") # Print MedAE\n",
        "            print(f\"  R-squared: {metrics['R-squared']:.2f}\" if not np.isnan(metrics['R-squared']) else \"  R-squared: Not applicable (target variable is constant in test set)\")\n",
        "            print(f\"  Mean Absolute Percentage Error (MAPE): {metrics['MAPE']:.2f}%\" if not np.isnan(metrics['MAPE']) else \"  Mean Absolute Percentage Error (MAPE): Not applicable (contains zero actual values)\")\n",
        "    else:\n",
        "        print(\"No models were successfully trained and evaluated.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Columns loaded: ['Unnamed: 0', 'key', 'fare_amount', 'pickup_datetime', 'pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude', 'passenger_count']\n",
            "\n",
            "First 5 rows of the DataFrame:\n",
            "   Unnamed: 0                            key  fare_amount  \\\n",
            "0    24238194    2015-05-07 19:52:06.0000003          7.5   \n",
            "1    27835199    2009-07-17 20:04:56.0000002          7.7   \n",
            "2    44984355   2009-08-24 21:45:00.00000061         12.9   \n",
            "3    25894730    2009-06-26 08:22:21.0000001          5.3   \n",
            "4    17610152  2014-08-28 17:47:00.000000188         16.0   \n",
            "\n",
            "           pickup_datetime  pickup_longitude  pickup_latitude  \\\n",
            "0  2015-05-07 19:52:06 UTC        -73.999817        40.738354   \n",
            "1  2009-07-17 20:04:56 UTC        -73.994355        40.728225   \n",
            "2  2009-08-24 21:45:00 UTC        -74.005043        40.740770   \n",
            "3  2009-06-26 08:22:21 UTC        -73.976124        40.790844   \n",
            "4  2014-08-28 17:47:00 UTC        -73.925023        40.744085   \n",
            "\n",
            "   dropoff_longitude  dropoff_latitude  passenger_count  \n",
            "0         -73.999512         40.723217                1  \n",
            "1         -73.994710         40.750325                1  \n",
            "2         -73.962565         40.772647                1  \n",
            "3         -73.965316         40.803349                3  \n",
            "4         -73.973082         40.761247                5  \n",
            "------------------------------\n",
            "Dropped 'Unnamed: 0' column.\n",
            "\n",
            "DataFrame size after cleaning and feature engineering: 193645 rows\n",
            "------------------------------\n",
            "Data split into 154916 training and 38729 testing samples.\n",
            "Training Linear Regression...\n",
            "Training Decision Tree...\n",
            "Training Random Forest...\n",
            "Training Gradient Boosting...\n",
            "\n",
            "============================================================\n",
            "Model Performance Comparison on Test Set\n",
            "============================================================\n",
            "\n",
            "Metrics Explanation:\n",
            "--------------------\n",
            "MAE (Mean Absolute Error): Average magnitude of prediction errors ($). Lower is better.\n",
            "MSE (Mean Squared Error): Average of squared prediction errors. Penalizes large errors more. Lower is better.\n",
            "RMSE (Root Mean Squared Error): Square root of MSE ($). Typical magnitude of prediction errors. Lower is better.\n",
            "MedAE (Median Absolute Error): Median magnitude of prediction errors ($). Robust to outliers. Lower is better.\n",
            "R-squared (R²): Proportion of variance in fare explained by the model (0 to 1). Higher is better.\n",
            "MAPE (Mean Absolute Percentage Error): Average percentage error relative to the actual fare (%). Lower is better.\n",
            "------------------------------------------------------------\n",
            "                    MAE    MSE  RMSE  MedAE  R-squared    MAPE\n",
            "Linear Regression  5.90  90.71  9.52   4.39       0.02  67.30%\n",
            "Decision Tree      3.01  48.58  6.97   1.60       0.47  43.35%\n",
            "Random Forest      2.16  20.55  4.53   1.28       0.78  28.56%\n",
            "Gradient Boosting  2.04  18.64  4.32   1.22       0.80  25.57%\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, median_absolute_error\n",
        "from sklearn.preprocessing import StandardScaler # Import StandardScaler\n",
        "import numpy as np\n",
        "import warnings\n",
        "\n",
        "# Ignore potential warnings from sklearn or pandas\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# --- File Loading ---\n",
        "# The filename is 'uber.csv'\n",
        "# Changed sep='\\t' to sep=',' because the output showed comma separation.\n",
        "try:\n",
        "    # Added low_memory=False to handle potential mixed data types in large files\n",
        "    df = pd.read_csv('uber.csv', sep=',', low_memory=False) # Changed separator here!\n",
        "\n",
        "    # --- Added lines to check the loaded data (keep these for verification) ---\n",
        "    print(\"Columns loaded:\", df.columns.tolist())\n",
        "    print(\"\\nFirst 5 rows of the DataFrame:\")\n",
        "    print(df.head())\n",
        "    print(\"-\" * 30) # Separator for clarity\n",
        "    # --- End of added lines ---\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'uber.csv' not found. Please make sure the CSV file is in the correct directory.\")\n",
        "    exit() # Exit if the file is not found\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while reading the CSV file: {e}\")\n",
        "    exit()\n",
        "\n",
        "# --- Preprocessing and Feature Engineering ---\n",
        "\n",
        "# Explicitly drop the 'Unnamed: 0' column if it exists (from previous saving)\n",
        "if 'Unnamed: 0' in df.columns:\n",
        "    df.drop('Unnamed: 0', axis=1, inplace=True)\n",
        "    print(\"Dropped 'Unnamed: 0' column.\")\n",
        "\n",
        "# Ensure essential columns exist before proceeding\n",
        "required_cols = ['fare_amount', 'pickup_datetime', 'pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude', 'passenger_count']\n",
        "if not all(col in df.columns for col in required_cols):\n",
        "    missing = [col for col in required_cols if col not in df.columns]\n",
        "    print(f\"Error: Missing required column(s) in the CSV file: {missing}\")\n",
        "    print(\"Please check the column names and separator in your CSV file.\")\n",
        "    exit()\n",
        "\n",
        "# Convert pickup_datetime to datetime objects\n",
        "df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'].astype(str).str.replace(' UTC', '', regex=False), errors='coerce')\n",
        "\n",
        "# Drop rows where pickup_datetime is NaT after coercion\n",
        "df.dropna(subset=['pickup_datetime'], inplace=True)\n",
        "\n",
        "# Extract time-based features\n",
        "df['year'] = df['pickup_datetime'].dt.year\n",
        "df['month'] = df['pickup_datetime'].dt.month\n",
        "df['day'] = df['pickup_datetime'].dt.day\n",
        "df['hour'] = df['pickup_datetime'].dt.hour\n",
        "df['day_of_week'] = df['pickup_datetime'].dt.dayofweek # Monday=0, Sunday=6\n",
        "df['day_of_year'] = df['pickup_datetime'].dt.dayofyear # Added Day of Year\n",
        "\n",
        "# Add cyclical features for hour and day_of_week\n",
        "df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
        "df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
        "df['dayofweek_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)\n",
        "df['dayofweek_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)\n",
        "\n",
        "\n",
        "# Calculate distance between pickup and dropoff coordinates\n",
        "coordinate_cols = ['pickup_latitude', 'pickup_longitude', 'dropoff_latitude', 'dropoff_longitude']\n",
        "# Check coordinate columns exist before applying - checked above, but double check here\n",
        "if not all(col in df.columns for col in coordinate_cols):\n",
        "     print(f\"Error: Missing one or more coordinate columns: {coordinate_cols} before distance calculation.\")\n",
        "     exit()\n",
        "\n",
        "def haversine_distance(lat1, lon1, lat2, lon2):\n",
        "    R = 6371  # Earth radius in kilometers\n",
        "    # Check for valid numbers before converting to radians\n",
        "    if pd.isna([lat1, lon1, lat2, lon2]).any():\n",
        "        return np.nan\n",
        "    # Add basic check for realistic coordinates (within a broad range)\n",
        "    # This is a very rough check, more precise checks are possible for specific regions\n",
        "    if not (-180 <= lon1 <= 180 and -90 <= lat1 <= 90 and -180 <= lon2 <= 180 and -90 <= lat2 <= 90):\n",
        "         return np.nan\n",
        "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
        "    dlon = lon2 - lon1\n",
        "    dlat = lat2 - lat1\n",
        "    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
        "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))\n",
        "    return R * c\n",
        "\n",
        "df['distance'] = df.apply(lambda row: haversine_distance(row['pickup_latitude'], row['pickup_longitude'], row['dropoff_latitude'], row['dropoff_longitude']), axis=1)\n",
        "\n",
        "# --- Basic Outlier and Data Cleaning ---\n",
        "# Remove rows with invalid fare amounts (<= 0)\n",
        "df = df[df['fare_amount'] > 0]\n",
        "\n",
        "# Remove rows with invalid passenger counts (0 or potentially very high - assuming max 6 for typical car)\n",
        "df = df[df['passenger_count'] > 0]\n",
        "# You might adjust the upper limit based on domain knowledge or data exploration\n",
        "# df = df[df['passenger_count'] <= 6] # Optional: Uncomment to limit passenger count\n",
        "\n",
        "# Remove rows with invalid distance (<= 0) or NaN distances from haversine calculation\n",
        "df = df[df['distance'] > 0] # Keep only trips with positive distance\n",
        "\n",
        "# Drop any rows that might still have NaNs after cleaning and feature engineering\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# Ensure the DataFrame is not empty after dropping NaNs and outliers\n",
        "if df.empty:\n",
        "    print(\"Error: DataFrame is empty after dropping rows with missing, invalid, or outlier data.\")\n",
        "    print(\"Consider reviewing the cleaning steps or the input data.\")\n",
        "    exit()\n",
        "else:\n",
        "    print(f\"\\nDataFrame size after cleaning and feature engineering: {len(df)} rows\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "# Updated features list to include cyclical features\n",
        "features = ['year', 'month', 'day', 'day_of_year', 'passenger_count', 'distance',\n",
        "            'hour_sin', 'hour_cos', 'dayofweek_sin', 'dayofweek_cos']\n",
        "target = 'fare_amount'\n",
        "\n",
        "# Ensure that the selected features and target columns exist in the DataFrame\n",
        "missing_features = [f for f in features if f not in df.columns]\n",
        "if missing_features:\n",
        "     print(f\"Error: Missing feature column(s) in DataFrame after preprocessing: {missing_features}\")\n",
        "     print(\"Please check your data and preprocessing steps.\")\n",
        "     exit()\n",
        "\n",
        "if target not in df.columns:\n",
        "    print(f\"Error: Target column '{target}' not found in the DataFrame after preprocessing.\")\n",
        "    print(\"Please check your data and preprocessing steps.\")\n",
        "    exit()\n",
        "\n",
        "\n",
        "X = df[features]\n",
        "y = df[target]\n",
        "\n",
        "# Split data into training and testing sets\n",
        "# We check if there's enough data to split\n",
        "if len(df) < 2:\n",
        "    print(\"Error: Not enough data to perform a train-test split. Need at least 2 data points.\")\n",
        "    exit()\n",
        "\n",
        "# Adjust test_size if the dataset is very small, ensuring at least one sample in each set\n",
        "test_size = 0.2\n",
        "if len(df) * test_size < 1 and len(df) > 1:\n",
        "    test_size = 1 / len(df) # Ensure at least one sample in test set if possible\n",
        "if len(df) * (1 - test_size) < 1 and len(df) > 1:\n",
        "     test_size = (len(df) - 1) / len(df) # Ensure at least one sample in training set if possible\n",
        "if test_size >= 1 or test_size <= 0: # Fallback for extremely small datasets (<=1)\n",
        "     if len(df) == 1:\n",
        "          print(\"Warning: Only one data point available. Cannot perform train-test split.\")\n",
        "          exit()\n",
        "     test_size = 0.5 # Default to 50/50 split for small >1 datasets\n",
        "\n",
        "\n",
        "try:\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
        "    print(f\"Data split into {len(X_train)} training and {len(X_test)} testing samples.\")\n",
        "except ValueError as e:\n",
        "    print(f\"Error during train-test split: {e}\")\n",
        "    print(\"This might happen if the dataset is too small or test_size calculation resulted in an invalid split.\")\n",
        "    exit()\n",
        "\n",
        "# --- Feature Scaling ---\n",
        "# Initialize StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the scaler on the training data and transform both training and testing data\n",
        "# This prevents data leakage from the test set into the scaling calculation\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Convert scaled arrays back to DataFrames (optional, but keeps structure)\n",
        "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=features, index=X_train.index)\n",
        "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=features, index=X_test.index)\n",
        "\n",
        "\n",
        "# Initialize and train different models\n",
        "models = {\n",
        "    \"Linear Regression\": LinearRegression(),\n",
        "    \"Decision Tree\": DecisionTreeRegressor(random_state=42), # Tree models are less sensitive to scaling\n",
        "    \"Random Forest\": RandomForestRegressor(random_state=42), # Tree models are less sensitive to scaling\n",
        "    \"Gradient Boosting\": GradientBoostingRegressor(random_state=42) # Tree models are less sensitive to scaling\n",
        "}\n",
        "\n",
        "results = {}\n",
        "\n",
        "# Check if test set is empty before making predictions\n",
        "if X_test_scaled_df.empty: # Use the scaled test set\n",
        "    print(\"Error: Test set is empty after splitting or scaling. Cannot evaluate models.\")\n",
        "else:\n",
        "    for name, model in models.items():\n",
        "        print(f\"Training {name}...\")\n",
        "        # Ensure there's enough data in training set\n",
        "        if X_train_scaled_df.empty or y_train.empty: # Use the scaled training set\n",
        "             print(f\"Error: Training set is empty for {name}. Cannot train model.\")\n",
        "             continue # Skip to the next model\n",
        "\n",
        "        # Train on scaled data\n",
        "        model.fit(X_train_scaled_df, y_train)\n",
        "        # Predict using scaled test data\n",
        "        y_pred = model.predict(X_test_scaled_df)\n",
        "\n",
        "        # Calculate metrics (using original y_test for evaluation)\n",
        "        mae = mean_absolute_error(y_test, y_pred)\n",
        "        mse = mean_squared_error(y_test, y_pred)\n",
        "        rmse = np.sqrt(mse)\n",
        "        medae = median_absolute_error(y_test, y_pred)\n",
        "\n",
        "        # Calculate R-squared\n",
        "        if np.std(y_test) == 0:\n",
        "            r2 = np.nan # R-squared is undefined if the target variable has no variance\n",
        "        else:\n",
        "            r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "        # Calculate Mean Absolute Percentage Error (MAPE)\n",
        "        # Avoid division by zero if actual fare is 0\n",
        "        y_test_no_zero = y_test[y_test != 0]\n",
        "        # Need to align y_pred with the filtered y_test\n",
        "        y_pred_no_zero = y_pred[y_test.index.isin(y_test_no_zero.index)]\n",
        "\n",
        "        if len(y_test_no_zero) > 0:\n",
        "             mape = np.mean(np.abs((y_test_no_zero - y_pred_no_zero) / y_test_no_zero)) * 100\n",
        "        else:\n",
        "             mape = np.nan # Cannot calculate MAPE if all actual values are zero\n",
        "\n",
        "\n",
        "        results[name] = {\n",
        "            \"MAE\": mae,\n",
        "            \"MSE\": mse,\n",
        "            \"RMSE\": rmse,\n",
        "            \"MedAE\": medae,\n",
        "            \"R-squared\": r2,\n",
        "            \"MAPE\": mape\n",
        "            }\n",
        "\n",
        "    # --- Print comparison of results with context ---\n",
        "    print(\"\\n\" + \"=\"*60) # Wider Separator\n",
        "    print(\"Model Performance Comparison on Test Set\")\n",
        "    print(\"=\"*60) # Wider Separator\n",
        "\n",
        "    print(\"\\nMetrics Explanation:\")\n",
        "    print(\"--------------------\")\n",
        "    print(\"MAE (Mean Absolute Error): Average magnitude of prediction errors ($). Lower is better.\")\n",
        "    print(\"MSE (Mean Squared Error): Average of squared prediction errors. Penalizes large errors more. Lower is better.\")\n",
        "    print(\"RMSE (Root Mean Squared Error): Square root of MSE ($). Typical magnitude of prediction errors. Lower is better.\")\n",
        "    print(\"MedAE (Median Absolute Error): Median magnitude of prediction errors ($). Robust to outliers. Lower is better.\")\n",
        "    print(\"R-squared (R²): Proportion of variance in fare explained by the model (0 to 1). Higher is better.\")\n",
        "    print(\"MAPE (Mean Absolute Percentage Error): Average percentage error relative to the actual fare (%). Lower is better.\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    if results: # Only print results if models were evaluated\n",
        "        # Create a pandas DataFrame for better comparison output\n",
        "        results_df = pd.DataFrame(results).T # Transpose to have models as rows\n",
        "        # Format the MAPE column as a string with '%' for better readability\n",
        "        results_df['MAPE'] = results_df['MAPE'].apply(lambda x: f\"{x:.2f}%\" if not np.isnan(x) else \"N/A\")\n",
        "        print(results_df.round(2)) # Print other metrics with 2 decimal places\n",
        "    else:\n",
        "        print(\"No models were successfully trained and evaluated.\")\n",
        "\n",
        "    print(\"=\"*60) # Bottom Separator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
